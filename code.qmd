---
title: "sl-project"
format: html
editor: visual
---

# Data Import

Load all required packages for data handling, preprocessing, model estimation, cross-validation and performance evaluation.

```{r}
#| label: load-pckg
pacman::p_load(DBI, RSQLite, tidyverse, lubridate, naniar, caret, Metrics)
```

Connect to the SQLite database, extract player-level information by joining player metadata with player attributes, and close the database connection.

```{r}
#| label: query-df
con <- dbConnect(RSQLite::SQLite(), dbname = "data/database.sqlite")

df_player_raw <- dbGetQuery(con, "SELECT * FROM Player JOIN Player_Attributes USING (player_api_id)")
names(df_player_raw) <- make.unique(names(df_player_raw))


dbDisconnect(con)
```

# Exploratory Analysis

Clean and prepare the player dataset for modeling and analysis: remove incomplete rows, compute player age at the time of each attribute measurement, drop non-predictive identifiers and metadata, and exclude observations with age \< 16 as we identified potentially erroneous records for very young players.

```{r}
#| label: prep-df
df_player <- df_player_raw %>%
  mutate(date = ymd_hms(date),
         birthday = ymd_hms(birthday),
         age = floor(time_length(interval(birthday, date),"years"))) %>%
  select(-id,-id.1,-player_name, -player_fifa_api_id, -player_fifa_api_id.1, -weight, -height, -birthday, -date, -defensive_work_rate, -attacking_work_rate, -overall_rating) %>%
  filter(age>=16)
```

```{r}
#| label: missings

gg_miss_var(df_player, show_pct = TRUE) +
  labs(title = "Percentage of Missing Values per Variable")
```

```{r}
#| label: corr-heatmap
target <- "potential"

cors <- df_player %>%
  select(where(is.numeric)) %>%
  summarise(across(-all_of(target),~ cor(.x, .data[[target]], use = "pairwise.complete.obs"))) %>%
  pivot_longer(everything(), names_to = "var", values_to = "r")

top5  <- cors %>% arrange(desc(r)) %>% slice(1:5)
flop5 <- cors %>% arrange(r) %>% slice(1:5)

vars <- c(target, top5$var, flop5$var)
num_df <- df_player %>% select(all_of(vars))

C <- cor(num_df, use = "pairwise.complete.obs")
as.data.frame(C) %>%
  tibble::rownames_to_column("Var1") %>%
  pivot_longer(-Var1, names_to = "Var2", values_to = "corr") %>%
  filter(!is.na(corr)) %>%
  ggplot(aes(Var2, Var1, fill = corr)) +
  geom_tile() +
  scale_fill_gradient2(limits = c(-1, 1)) +
  coord_fixed() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = NULL, y = NULL, fill = "r",title = "Top and Bottom Correlates of Player Potential")
```

Construct the design matrix with one-hot encoding for categorical variables and define the target variable (potential).

```{r}
#| label: model-matrix
df_player <- df_player %>% filter(complete.cases(.))

X <- model.matrix(potential~.,df_player)[,-1]
y <- df_player$potential
```

# Task 1 - Deep Feedforward Neural Network

# Task 2 - Baseline Comparison with Traditional SL Models

We estimate a regularized baseline model for predicting player potential and evaluate its out-of-sample performance using grouped 10-fold cross-validation. Specifically, we employ a Lasso regression, which provides a natural extension of the linear model by introducing $l_1$-regularization to control overfitting and perform implicit feature selection.

To prevent information leakage arising from repeated observations of the same player, we, once again, construct group-wise folds based on player identifiers, ensuring that all observations of a given player are assigned to the same fold. Within each training fold, the regularization parameter is selected via internal cross-validation, and the model is then estimated on the corresponding training data.

Predictions are generated for both training and test folds, and model performance is assessed using mean squared error (MSE) and mean absolute error (MAE). Finally, results are averaged across folds to obtain a robust estimate of the model’s generalization performance.

```{r}
#| label: lasso_model
#| message: false
# Set up 10-fold cross validation indices
set.seed(123)

player_id <- df_player$player_api_id
X <- X[, !colnames(X) %in% c("player_api_id","player_fifa_api_id.1")]
folds <- groupKFold(group=player_id, k = 10)

mse_train <- c()
mse_test  <- c()
mae_train <- c()
mae_test  <- c()

predictions <- tibble()

# Perform 10-fold CV
for (i in 1:10) {

  train_idx <- folds[[i]]
  test_idx  <- setdiff(seq_len(nrow(X)), train_idx)

  X_train <- X[train_idx, ]
  X_test  <- X[test_idx, ]
  y_train <- y[train_idx]
  y_test  <- y[test_idx]

  # Model matrices (glmnet will standardize internally)
  X_train_mm <- model.matrix(~ . - 1, data = as.data.frame(X_train))
  X_test_mm  <- model.matrix(~ . - 1, data = as.data.frame(X_test))

  cv_fit <- cv.glmnet(x = X_train_mm,y = y_train, alpha = 1, standardize = TRUE)

  # ---- Predict ----
  y_pred_train <- as.numeric(predict(cv_fit, newx = X_train_mm, s = "lambda.min"))
  y_pred_test  <- as.numeric(predict(cv_fit, newx = X_test_mm,  s = "lambda.min"))
  predictions <- bind_rows(predictions,tibble(fold = i, player_api_id = player_id[test_idx],   actual = y_test,predicted = y_pred_test))

  # metrics
  mse_train[i] <- mse(y_train, y_pred_train)
  mse_test[i]  <- mse(y_test, y_pred_test)

  mae_train[i] <- mae(y_train, y_pred_train)
  mae_test[i]  <- mae(y_test, y_pred_test)}

# results using tidyverse tibble
cv_results <- tibble(
  Fold      = 1:10,
  Train_MSE = mse_train,
  Test_MSE  = mse_test,
  Train_MAE = mae_train,
  Test_MAE  = mae_test)

cv_results

# averaged over all folds
kfold_summary <- tibble(
  Mean_Train_MSE = mean(mse_train),
  Mean_Test_MSE  = mean(mse_test),
  Mean_Train_MAE = mean(mae_train),
  Mean_Test_MAE  = mean(mae_test))

kfold_summary
```

The results indicate highly stable performance of the lasso model across all 10 folds. Training and test errors are consistently close in each fold, suggesting no evidence of overfitting. Test MSE varies only moderately across folds (around 15.78–15.88), while MAE remains tightly clustered around 3. This limited variability across folds indicates robust generalization performance of the baseline model.

## Model Diagnostics

To assess the robustness of the baseline linear regression model, we conduct a series of diagnostic checks examining key model assumptions and the stability of predictive performance.

### Linearity Check

```{r}
#| label: linearity check
#| message: false
#| fig.height: 6
#| fig.width: 8
predictors <- colnames(X)

df_plot <- as_tibble(X) %>% 
  mutate(potential = y)

df_long <- df_plot %>% 
  pivot_longer(
    cols = all_of(predictors),
    names_to = "predictor",
    values_to = "value")

ggplot(df_long, aes(x = value, y = potential)) +
  geom_smooth(method = "lm",color="black") +
  facet_wrap(~ predictor, scales = "free_x") +
  labs(title = "Potential vs. Predictors",y = "Potential",x = NULL) +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

The diagnostic plots indicate largely monotonic and approximately linear relationships between player potential and the majority of predictors. While the strength of associations varies across attributes, no clear nonlinear patterns or threshold effects are observed. Overall, the linearity assumption appears reasonably satisfied, supporting the use of a linear regression model as an appropriate baseline.

### Homoskedasticity Check

```{r}
#| label: homoskedasticity check
model_full <- lm(y ~ ., data = as.data.frame(X))

df_resid <- tibble(fitted = fitted(model_full),resid  = resid(model_full))

ggplot(df_resid, aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Residuals vs. Fitted Values",x = "Fitted values",y = "Residuals") +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

The residuals-versus-fitted plot shows that residuals are centered around zero with a broadly constant spread across the range of fitted values. While a slight increase in variance is visible in the mid-left-range, no clear funneling patten emerges. Overall, deviations from homoskedasticity appear mild and are unlikely to affect predictive performance.

### Normality of Residuals

```{r}
#| label: homoskedasticity check
df_qq <- tibble(resid = resid(model_full))

ggplot(df_qq, aes(sample = resid)) +
  stat_qq(alpha = 0.3) +
  stat_qq_line(color = "red") +
  labs(title = "Normal Q–Q Plot of Residuals", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

The Q–Q plot of the residuals indicates approximate normality, with observations closely following the reference line in the central region. Deviations are visible in the tails, suggesting slightly heavier tails than a normal distribution, which is expected given the large sample size. These deviations are unlikely to affect the model’s predictive performance massivly.
