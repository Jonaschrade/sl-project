---
title: "sl-project"
format: html
editor: visual
---

# Data Import and Preprocessing

Load all required packages for data handling, preprocessing, model estimation, cross-validation and performance evaluation.

```{r}
#| label: load-pckg
pacman::p_load(DBI, RSQLite, tidyverse, lubridate, caret, Metrics)
```

Connect to the SQLite database, extract player-level information by joining player metadata with player attributes, and close the database connection.

```{r}
#| label: query-df
con <- dbConnect(RSQLite::SQLite(), dbname = "data/database.sqlite")

df_player_raw <- dbGetQuery(con, "SELECT * FROM Player JOIN Player_Attributes USING (player_api_id)")
names(df_player_raw) <- make.unique(names(df_player_raw))


dbDisconnect(con)
```

Clean and prepare the player dataset for modeling: remove incomplete rows, compute player age at the time of each attribute measurement, drop non-predictive identifiers and metadata, and exclude observations with age \< 16 as we identified potentially erroneous records for very young players. Then construct the design matrix with one-hot encoding for categorical variables and define the target variable (potential).

```{r}
#| label: prep-df
df_player <- df_player_raw %>%
  filter(complete.cases(.)) %>%
  mutate(date = ymd_hms(date),
         birthday = ymd_hms(birthday),
         age = floor(time_length(interval(birthday, date),"years"))) %>%
  select(-id,-id.1,-player_name, -player_fifa_api_id, -weight, -height, -birthday, -date, -defensive_work_rate, -attacking_work_rate, -overall_rating) %>%
  filter(age>=16)

X <- model.matrix(potential~.,df_player)[,-1]
y <- df_player$potential
```

# Task 1 - Deep Feedforward Neural Network

# Task 2 - Baseline Comparison with Traditional SL Models

Estimate a baseline model for predicting player potential and evaluate its out-of-sample performance using grouped 5-fold cross-validation. We choose ordinary least squares linear regression as the baseline because it is a standard, interpretable reference model for continuous outcomes and allows a fair comparison to the neural network: it uses the same input information but imposes a simple functional form. To avoid information leakage when the same player appears multiple times, we once again construct group-wise folds based on each player's id so that all observations of a player remain within the same fold. Within each fold, we standardize predictors using only the training data and apply the same transformation to the test data. We then fit the linear model on the training fold and compute MSE and MAE on both training and test folds; finally, we summarize results across folds to obtain a robust estimate of generalization performance.

```{r}
#| label: linear_model
#| message: false
# Set up 5-fold cross validation indices
set.seed(123)

player_id <- df_player$player_api_id
X <- X[, !colnames(X) %in% c("player_api_id","player_fifa_api_id.1")]
folds <- groupKFold(group=player_id, k = 5)

mse_train <- c()
mse_test  <- c()
mae_train <- c()
mae_test  <- c()

predictions <- tibble()

# Perform 5-fold CV
for (i in 1:5) {

  train_idx <- folds[[i]]
  test_idx  <- setdiff(seq_len(nrow(X)), train_idx)

  X_train <- X[train_idx, ]
  X_test  <- X[test_idx, ]
  y_train <- y[train_idx]
  y_test  <- y[test_idx]

  # z-score within this fold
  preproc <- preProcess(X_train, method = c("center", "scale"))
  X_train_scaled <- predict(preproc, X_train)
  X_test_scaled  <- predict(preproc, X_test)

  # fit
  model <- lm(y_train ~ ., data = as.data.frame(X_train_scaled))

  # predict
  y_pred_train <- predict(model, newdata = as.data.frame(X_train_scaled))
  y_pred_test  <- predict(model, newdata = as.data.frame(X_test_scaled))
  predictions <- bind_rows(predictions,tibble(fold = i, player_api_id = player_id[test_idx], actual = y_test,predicted = y_pred_test))

  # metrics
  mse_train[i] <- mse(y_train, y_pred_train)
  mse_test[i]  <- mse(y_test, y_pred_test)

  mae_train[i] <- mae(y_train, y_pred_train)
  mae_test[i]  <- mae(y_test, y_pred_test)}

# results using tidyverse tibble
cv_results <- tibble(
  Fold      = 1:5,
  Train_MSE = mse_train,
  Test_MSE  = mse_test,
  Train_MAE = mae_train,
  Test_MAE  = mae_test)

cv_results

# averaged over all folds
kfold_summary <- tibble(
  Mean_Train_MSE = mean(mse_train),
  Mean_Test_MSE  = mean(mse_test),
  Mean_Train_MAE = mean(mae_train),
  Mean_Test_MAE  = mean(mae_test))

kfold_summary
```

The results indicate highly stable performance of the linear regression model across all five folds. Training and test errors are consistently close in each fold, suggesting no evidence of overfitting. Test MSE varies only moderately across folds (around 15.1–16.2), while MAE remains tightly clustered around 3. This limited variability across folds indicates robust generalization performance of the baseline model.

## Model Diagnostics

To assess the robustness of the baseline linear regression model, we conduct a series of diagnostic checks examining key model assumptions and the stability of predictive performance.

### Linearity Check

```{r}
#| label: linearity check
#| message: false
#| fig.height: 6
#| fig.width: 8
predictors <- colnames(X)

df_plot <- as_tibble(X) %>% 
  mutate(potential = y)

df_long <- df_plot %>% 
  pivot_longer(
    cols = all_of(predictors),
    names_to = "predictor",
    values_to = "value")

ggplot(df_long, aes(x = value, y = potential)) +
  geom_smooth(method = "lm",color="black") +
  facet_wrap(~ predictor, scales = "free_x") +
  labs(title = "Potential vs. Predictors",y = "Potential",x = NULL) +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

The diagnostic plots indicate largely monotonic and approximately linear relationships between player potential and the majority of predictors. While the strength of associations varies across attributes, no clear nonlinear patterns or threshold effects are observed. Overall, the linearity assumption appears reasonably satisfied, supporting the use of a linear regression model as an appropriate baseline.

### Homoskedasticity Check

```{r}
#| label: homoskedasticity check
model_full <- lm(y ~ ., data = as.data.frame(X))

df_resid <- tibble(fitted = fitted(model_full),resid  = resid(model_full))

ggplot(df_resid, aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Residuals vs. Fitted Values",x = "Fitted values",y = "Residuals") +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

The residuals-versus-fitted plot shows that residuals are centered around zero with a broadly constant spread across the range of fitted values. While a slight increase in variance is visible in the mid-left-range, no clear funneling patten emerges. Overall, deviations from homoskedasticity appear mild and are unlikely to affect predictive performance.

### Normality of Residuals

```{r}
#| label: homoskedasticity check
df_qq <- tibble(resid = resid(model_full))

ggplot(df_qq, aes(sample = resid)) +
  stat_qq(alpha = 0.3) +
  stat_qq_line(color = "red") +
  labs(title = "Normal Q–Q Plot of Residuals", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

The Q–Q plot of the residuals indicates approximate normality, with observations closely following the reference line in the central region. Deviations are visible in the tails, suggesting slightly heavier tails than a normal distribution, which is expected given the large sample size. These deviations are unlikely to affect the model’s predictive performance massivly.
